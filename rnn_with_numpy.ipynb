{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tmax</th>\n",
       "      <th>tmin</th>\n",
       "      <th>rain</th>\n",
       "      <th>tmax_tomorrow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1970-01-01</th>\n",
       "      <td>60.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-02</th>\n",
       "      <td>52.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-03</th>\n",
       "      <td>52.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-04</th>\n",
       "      <td>53.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-05</th>\n",
       "      <td>52.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-06</th>\n",
       "      <td>50.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-07</th>\n",
       "      <td>52.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-08</th>\n",
       "      <td>56.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.24</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-09</th>\n",
       "      <td>54.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-10</th>\n",
       "      <td>57.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            tmax  tmin  rain  tmax_tomorrow\n",
       "1970-01-01  60.0  35.0  0.00           52.0\n",
       "1970-01-02  52.0  39.0  0.00           52.0\n",
       "1970-01-03  52.0  35.0  0.00           53.0\n",
       "1970-01-04  53.0  36.0  0.00           52.0\n",
       "1970-01-05  52.0  35.0  0.00           50.0\n",
       "1970-01-06  50.0  38.0  0.00           52.0\n",
       "1970-01-07  52.0  43.0  0.00           56.0\n",
       "1970-01-08  56.0  49.0  0.24           54.0\n",
       "1970-01-09  54.0  50.0  0.40           57.0\n",
       "1970-01-10  57.0  50.0  0.00           57.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read in our data, and fill missing values\n",
    "data = pd.read_csv(\"./data/clean_weather.csv\", index_col=0)\n",
    "data = data.ffill()\n",
    "\n",
    "# Display a sequence of temperatures\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1970-01-01    60.0\n",
       "1970-01-02    52.0\n",
       "1970-01-03    52.0\n",
       "1970-01-04    53.0\n",
       "1970-01-05    52.0\n",
       "1970-01-06    50.0\n",
       "1970-01-07    52.0\n",
       "1970-01-08    56.0\n",
       "1970-01-09    54.0\n",
       "1970-01-10    57.0\n",
       "Name: tmax, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"tmax\"].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Architecture\n",
    "\n",
    "![img](https://xiaophai-typora.oss-cn-shanghai.aliyuncs.com/2256672-cf18bb1f06e750a4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Forward Pass\n",
    "\n",
    "### Initialize weight matrixes\n",
    "\n",
    "$$\n",
    "input\\ layer: U_{1\\times5}, \\qquad recurrent\\ layer: W_{5\\times5}, b_{1\\times5} \\qquad output\\ layer: V_{5\\times1}, b_{1\\times1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# Define our weights and biases\n",
    "# Scale them down so values get through the tanh nonlinearity\n",
    "i_weight = np.random.rand(1,5) / 5 - .1\n",
    "\n",
    "h_weight = np.random.rand(5,5) / 5 - .1\n",
    "h_bias = np.random.rand(1,5) / 5 - .1\n",
    "\n",
    "# Tanh pushes values to between -1 and 1, so scale up the output weights\n",
    "o_weight = np.random.rand(5,1) * 50\n",
    "o_bias = np.random.rand(1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo\n",
    "\n",
    "#### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This will store the previous hidden state, since we'll need it to calculate the current hidden step\n",
    "prev_hidden = None\n",
    "sequence = data[\"tmax\"].tail(3).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An array to store the output predictions\n",
    "outputs = np.zeros(3)\n",
    "# An array to store hidden states for use in backpropagation\n",
    "hiddens = np.zeros((3, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xiaophai\\AppData\\Local\\Temp\\ipykernel_28124\\4045844613.py:20: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  outputs[i] = xo\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(3):\n",
    "    # Get the input sequence at the given position\n",
    "    x = sequence[i].reshape(1,1)\n",
    "\n",
    "    # Multiply input by input weight\n",
    "    xi = x @ i_weight\n",
    "    if prev_hidden is not None:\n",
    "        # Add previous hidden to input\n",
    "        xh = xi + prev_hidden @ h_weight + h_bias\n",
    "    else:\n",
    "        xh = xi\n",
    "\n",
    "    # Apply our activation function\n",
    "    xh = np.tanh(xh)\n",
    "    prev_hidden = xh\n",
    "    hiddens[i,] = xh\n",
    "\n",
    "    # Multiply by the output weight\n",
    "    xo = xh @ o_weight + o_bias\n",
    "    outputs[i] = xo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([74.31470595, 80.66149404, 77.67852446])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Calculating loss\n",
    "\n",
    "$$\n",
    "loss = \\frac{1}{2}\\sum_i (actual_i - predict_i)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "loss\\_grad_i = \\frac{\\partial loss}{\\partial predict_i} = (predict_i - actual_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(actual, predicted):\n",
    "    return np.mean((actual-predicted)**2)\n",
    "\n",
    "def mse_grad(actual, predicted):\n",
    "    return (predicted - actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.31470595, 18.66149404, 12.67852446])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Actual next day temperatures\n",
    "actuals = np.array([70, 62, 65])\n",
    "\n",
    "loss_grad = mse_grad(actuals, outputs)\n",
    "loss_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial E^t}{\\partial W}\n",
    "% 第一个等号\n",
    "&=\n",
    "\\frac{\\partial E^t}{\\partial \\boldsymbol{\\eta}^t}\n",
    "\\frac{\\partial \\boldsymbol{\\eta}^t}{\\partial W}\n",
    "\\\\\n",
    "&=\n",
    "\\frac{\\partial E^t}{\\partial \\boldsymbol{\\eta}^t}\n",
    "\\frac{\\partial W}{\\partial W}\\boldsymbol{s}^{t-1}\n",
    "+\n",
    "\\frac{\\partial E^t}{\\partial \\boldsymbol{\\eta}^{t-1}}\n",
    "\\frac{\\partial \\boldsymbol{\\eta}^{t-1}}{\\partial W}\n",
    "\\\\\n",
    "&=\n",
    "\\frac{\\partial E^t}{\\partial \\boldsymbol{\\eta}^t}\n",
    "\\frac{\\partial W}{\\partial W}\\boldsymbol{s}^{t-1}\n",
    "+\n",
    "\\frac{\\partial E^t}{\\partial \\boldsymbol{\\eta}^{t-1}}\n",
    "\\frac{\\partial W}{\\partial W}\\boldsymbol{s}^{t-2}\n",
    "+\n",
    "\\frac{\\partial E^t}{\\partial \\boldsymbol{\\eta}^{t-2}}\n",
    "\\frac{\\partial \\boldsymbol{\\eta}^{t-2}}{\\partial W}\n",
    "\\\\\n",
    "&=\n",
    "\\frac{\\partial E^t}{\\partial \\boldsymbol{\\eta}^t}\n",
    "\\frac{\\partial W}{\\partial W}\\boldsymbol{s}^{t-1}\n",
    "+\n",
    "\\frac{\\partial E^t}{\\partial \\boldsymbol{\\eta}^{t-1}}\n",
    "\\frac{\\partial W}{\\partial W}\\boldsymbol{s}^{t-2}\n",
    "+\n",
    "\\frac{\\partial E^t}{\\partial \\boldsymbol{\\eta}^{t-2}}\n",
    "\\frac{\\partial W}{\\partial W}\\boldsymbol{s}^{t-3}\n",
    "+\n",
    "\\cdots\n",
    "+\n",
    "\\frac{\\partial E^t}{\\partial \\boldsymbol{\\eta}^{2}}\n",
    "\\frac{\\partial W}{\\partial W}\\boldsymbol{s}^{1}\n",
    "+\n",
    "\\frac{\\partial E^t}{\\partial \\boldsymbol{\\eta}^{1}}\n",
    "\\frac{\\partial W}{\\partial W}\\boldsymbol{s}^{0}\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial E^t}{\\partial U}\n",
    "% 第一个等号\n",
    "&=\n",
    "\\frac{\\partial E^t}{\\partial \\boldsymbol{\\eta}^t}\n",
    "\\frac{\\partial \\boldsymbol{\\eta}^t}{\\partial U}\n",
    "\\\\\n",
    "&=\n",
    "\\frac{\\partial E^t}{\\partial \\boldsymbol{\\eta}^t}\n",
    "\\frac{\\partial U\\boldsymbol{x}^t}{\\partial U}\n",
    "+\n",
    "\\frac{\\partial E^t}{\\partial \\boldsymbol{\\eta}^{t-1}}\n",
    "\\frac{\\partial \\boldsymbol{\\eta}^{t-1}}{\\partial U}\n",
    "\\\\\n",
    "&=\n",
    "\\frac{\\partial E^t}{\\partial \\boldsymbol{\\eta}^t}\n",
    "\\frac{\\partial U\\boldsymbol{x}^t}{\\partial U}\n",
    "+\n",
    "\\frac{\\partial E^t}{\\partial \\boldsymbol{\\eta}^{t-1}}\n",
    "\\frac{\\partial U\\boldsymbol{x}^{t-1}}{\\partial U}\n",
    "+\n",
    "\\frac{\\partial E^t}{\\partial \\boldsymbol{\\eta}^{t-2}}\n",
    "\\frac{\\partial \\boldsymbol{\\eta}^{t-2}}{\\partial U}\n",
    "\\\\\n",
    "&=\n",
    "\\frac{\\partial E^t}{\\partial \\boldsymbol{\\eta}^t}\n",
    "\\frac{\\partial U\\boldsymbol{x}^t}{\\partial U}\n",
    "+\n",
    "\\frac{\\partial E^t}{\\partial \\boldsymbol{\\eta}^{t-1}}\n",
    "\\frac{\\partial U\\boldsymbol{x}^{t-1}}{\\partial U}\n",
    "+\n",
    "\\frac{\\partial E^t}{\\partial \\boldsymbol{\\eta}^{t-2}}\n",
    "\\frac{\\partial U\\boldsymbol{x}^{t-2}}{\\partial U}\n",
    "+\n",
    "\\cdots\n",
    "+\n",
    "\\frac{\\partial E^t}{\\partial \\boldsymbol{\\eta}^{2}}\n",
    "\\frac{\\partial U\\boldsymbol{x}^{2}}{\\partial U}\n",
    "+\n",
    "\\frac{\\partial E^t}{\\partial \\boldsymbol{\\eta}^{1}}\n",
    "\\frac{\\partial U\\boldsymbol{x}^{1}}{\\partial U}\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_hidden = None\n",
    "\n",
    "o_weight_grad, o_bias_grad, h_weight_grad, h_bias_grad, i_weight_grad = [0] * 5\n",
    "\n",
    "for i in range(2, -1, -1):\n",
    "    l_grad = loss_grad[i].reshape(1,1)\n",
    "\n",
    "    o_weight_grad += hiddens[i][:,np.newaxis] @ l_grad\n",
    "    o_bias_grad += np.mean(l_grad)\n",
    "\n",
    "    o_grad = l_grad @ o_weight.T\n",
    "\n",
    "    # Only add in the hidden gradient if a next sequence exists\n",
    "    if next_hidden is not None:\n",
    "        h_grad = o_grad + next_hidden @ h_weight.T\n",
    "    else:\n",
    "        h_grad = o_grad\n",
    "\n",
    "    tanh_deriv = 1 - hiddens[i,:][np.newaxis,:] ** 2\n",
    "    h_grad = np.multiply(h_grad, tanh_deriv)\n",
    "\n",
    "    next_hidden = h_grad\n",
    "\n",
    "    # Don't update the hidden weights for the first sequence position\n",
    "    if i > 0:\n",
    "        h_weight_grad += hiddens[i-1,:][:,np.newaxis] @ h_grad\n",
    "        h_bias_grad += np.mean(h_grad)\n",
    "\n",
    "    i_weight_grad += sequence[i].reshape(1,1).T @ h_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use gradient descent to make parameter updates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-6\n",
    "# We'll divide the learning rate by the sequence length, since we were adding together the gradients\n",
    "# This makes training the model more stable\n",
    "lr = lr / 3\n",
    "\n",
    "i_weight -= i_weight_grad * lr\n",
    "h_weight -= h_weight_grad * lr\n",
    "h_bias -= h_bias_grad * lr\n",
    "o_weight -= o_weight_grad * lr\n",
    "o_bias -= o_bias_grad * lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Full Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know enough to do a full implementation of a complete network.  This code will mostly be the same as the forward and backward passes we already implemented.\n",
    "\n",
    "We'll first load in and scale our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xiaophai\\.conda\\envs\\python39\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "\n",
    "# Define predictors and target\n",
    "PREDICTORS = [\"tmax\", \"tmin\", \"rain\"]\n",
    "TARGET = \"tmax_tomorrow\"\n",
    "\n",
    "# Scale our data to have mean 0\n",
    "scaler = StandardScaler()\n",
    "data[PREDICTORS] = scaler.fit_transform(data[PREDICTORS])\n",
    "\n",
    "# Split into train, valid, test sets\n",
    "np.random.seed(0)\n",
    "split_data = np.split(data, [int(.7*len(data)), int(.85*len(data))])\n",
    "(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = [[d[PREDICTORS].to_numpy(), d[[TARGET]].to_numpy()] for d in split_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Then we can initialize our weights and biases.  We'll scale our parameters so they are relatively small.  This helps the network descend better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(layer_conf):\n",
    "    layers = []\n",
    "    for i in range(1, len(layer_conf)):\n",
    "        np.random.seed(0)\n",
    "        k = 1/math.sqrt(layer_conf[i][\"hidden\"])\n",
    "        i_weight = np.random.rand(layer_conf[i-1][\"units\"], layer_conf[i][\"hidden\"]) * 2 * k - k\n",
    "\n",
    "        h_weight = np.random.rand(layer_conf[i][\"hidden\"], layer_conf[i][\"hidden\"]) * 2 * k - k\n",
    "        h_bias = np.random.rand(1, layer_conf[i][\"hidden\"]) * 2 * k - k\n",
    "\n",
    "        o_weight = np.random.rand(layer_conf[i][\"hidden\"], layer_conf[i][\"output\"]) * 2 * k - k\n",
    "        o_bias = np.random.rand(1, layer_conf[i][\"output\"]) * 2 * k - k\n",
    "\n",
    "        layers.append(\n",
    "            [i_weight, h_weight, h_bias, o_weight, o_bias]\n",
    "        )\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Then we'll write a forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, layers):\n",
    "    hiddens = []\n",
    "    outputs = []\n",
    "    for i in range(len(layers)):\n",
    "        i_weight, h_weight, h_bias, o_weight, o_bias = layers[i]\n",
    "        hidden = np.zeros((x.shape[0], i_weight.shape[1]))\n",
    "        output = np.zeros((x.shape[0], o_weight.shape[1]))\n",
    "        for j in range(x.shape[0]):\n",
    "            input_x = x[j,:][np.newaxis,:] @ i_weight\n",
    "            hidden_x = input_x + hidden[max(j-1,0),:][np.newaxis,:] @ h_weight + h_bias\n",
    "            # Activation.  tanh avoids outputs getting larger and larger.\n",
    "            hidden_x = np.tanh(hidden_x)\n",
    "            # Store hidden for use in backprop\n",
    "            hidden[j,:] = hidden_x\n",
    "\n",
    "            # Output layer\n",
    "            output_x = hidden_x @ o_weight + o_bias\n",
    "            output[j,:] = output_x\n",
    "        hiddens.append(hidden)\n",
    "        outputs.append(output)\n",
    "    return hiddens, outputs[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "And a backward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(layers, x, lr, grad, hiddens):\n",
    "    for i in range(len(layers)):\n",
    "        i_weight, h_weight, h_bias, o_weight, o_bias = layers[i]\n",
    "        hidden = hiddens[i]\n",
    "        next_h_grad = None\n",
    "        i_weight_grad, h_weight_grad, h_bias_grad, o_weight_grad, o_bias_grad = [0] * 5\n",
    "\n",
    "        for j in range(x.shape[0] - 1, -1, -1):\n",
    "            # Add newaxis in the first dimension\n",
    "            out_grad = grad[j,:][np.newaxis, :]\n",
    "\n",
    "            # Output updates\n",
    "            # np.newaxis creates a size 1 axis, in this case transposing matrix\n",
    "            o_weight_grad += hidden[j,:][:, np.newaxis] @ out_grad\n",
    "            o_bias_grad += out_grad\n",
    "\n",
    "            # Propagate gradient to hidden unit\n",
    "            h_grad = out_grad @ o_weight.T\n",
    "\n",
    "            if j < x.shape[0] - 1:\n",
    "                # Then we multiply the gradient by the hidden weights to pull gradient from next hidden state to current hidden state\n",
    "                hh_grad = next_h_grad @ h_weight.T\n",
    "                # Add the gradients together to combine output contribution and hidden contribution\n",
    "                h_grad += hh_grad\n",
    "\n",
    "            # Pull the gradient across the current hidden nonlinearity\n",
    "            # derivative of tanh is 1 - tanh(x) ** 2\n",
    "            # So we take the output of tanh (next hidden state), and plug in\n",
    "            tanh_deriv = 1 - hidden[j][np.newaxis,:] ** 2\n",
    "\n",
    "            # next_h_grad @ np.diag(tanh_deriv_next) multiplies each element of next_h_grad by the deriv\n",
    "            # Effect is to pull value across nonlinearity\n",
    "            h_grad = np.multiply(h_grad, tanh_deriv)\n",
    "\n",
    "            # Store to compute h grad for previous sequence position\n",
    "            next_h_grad = h_grad.copy()\n",
    "\n",
    "            # If we're not at the very beginning\n",
    "            if j > 0:\n",
    "                # Multiply input from previous layer by post-nonlinearity grad at current layer\n",
    "                h_weight_grad += hidden[j-1][:, np.newaxis] @ h_grad\n",
    "                h_bias_grad += h_grad\n",
    "\n",
    "            i_weight_grad += x[j,:][:,np.newaxis] @ h_grad\n",
    "\n",
    "        # Normalize lr by number of sequence elements\n",
    "        lr = lr / x.shape[0]\n",
    "        i_weight -= i_weight_grad * lr\n",
    "        h_weight -= h_weight_grad * lr\n",
    "        h_bias -= h_bias_grad * lr\n",
    "        o_weight -= o_weight_grad * lr\n",
    "        o_bias -= o_bias_grad * lr\n",
    "        layers[i] = [i_weight, h_weight, h_bias, o_weight, o_bias]\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can end by setting up a training loop and measuring error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 train loss 3122.5944001445105 valid loss 2171.3186862102048\n",
      "Epoch: 10 train loss 84.78519304952819 valid loss 81.34087884698089\n",
      "Epoch: 20 train loss 52.841666692401546 valid loss 52.309411675624744\n",
      "Epoch: 30 train loss 34.89142680576197 valid loss 34.42998362512595\n",
      "Epoch: 40 train loss 33.90674797419208 valid loss 33.78966022195152\n",
      "Epoch: 50 train loss 30.593193275313336 valid loss 30.568271740103118\n",
      "Epoch: 60 train loss 27.767753755456212 valid loss 27.466770094462962\n",
      "Epoch: 70 train loss 27.987707979297117 valid loss 27.581650000895717\n",
      "Epoch: 80 train loss 28.42712118570578 valid loss 27.791959394169872\n",
      "Epoch: 90 train loss 26.598198926416888 valid loss 25.80674232379205\n",
      "Epoch: 100 train loss 25.263986813543617 valid loss 24.43551751035542\n"
     ]
    }
   ],
   "source": [
    "epochs = 101\n",
    "lr = 1e-5\n",
    "\n",
    "layer_conf = [\n",
    "    {\"type\":\"input\", \"units\": 3},\n",
    "    {\"type\": \"rnn\", \"hidden\": 4, \"output\": 1}\n",
    "]\n",
    "layers = init_params(layer_conf)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    sequence_len = 7\n",
    "    epoch_loss = 0\n",
    "    for j in range(train_x.shape[0] - sequence_len):\n",
    "        seq_x = train_x[j:(j+sequence_len),]\n",
    "        seq_y = train_y[j:(j+sequence_len),]\n",
    "        hiddens, outputs = forward(seq_x, layers)\n",
    "        grad = mse_grad(seq_y, outputs)\n",
    "        params = backward(layers, seq_x, lr, grad, hiddens)\n",
    "        epoch_loss += mse(seq_y, outputs)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        sequence_len = 7\n",
    "        valid_loss = 0\n",
    "        for j in range(valid_x.shape[0] - sequence_len):\n",
    "            seq_x = valid_x[j:(j+sequence_len),]\n",
    "            seq_y = valid_y[j:(j+sequence_len),]\n",
    "            _, outputs = forward(seq_x, layers)\n",
    "            valid_loss += mse(seq_y, outputs)\n",
    "\n",
    "        print(f\"Epoch: {epoch} train loss {epoch_loss / len(train_x)} valid loss {valid_loss / len(valid_x)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
